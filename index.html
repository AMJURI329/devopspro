LiNUX
ques.Create the following users,groups and group membership
-A group named sysadm
-A user "harry" who belongs to sysadm as a secondary group.
-A user "natasha" who belongs to sysadm as a secondary group.
-A user "sarah" who does not have the access to an interactive shell and who is not a member of sysadm group.

-"harry" "natasha"  and "sarah" should all have password of password 
SOl:-
cat /etc/group
groupadd sysadm
cat /etc/group/ -i sysadm
useradd harry
passwd harry same for natasha and sarah
usermod -G sysadm harry
usermod -G sysadm natasha
usermod -s /sbin/nologin sarah
cat /etc/passwd | grep -i sarah
####################################################################################################################
ques.create a collabrative directory /shared/sysadm with th following characteristics:
-Group owernship of /shared/sysadm is sysadm
-The directory should be readable , writable and accesiable to member of sysadm . but not to any other user.
-Files created in /shared/sysadm automatically have group owernshipset to the sysadm group
sol:-
mkdir -p /shared/sysadm
ll -d /shared/sysadm
chgrp sysadm /shared/sysadm
ll -d /shared/syadm
chmod 770 /shared/sysadm
ll -d /shared/syadm
chmod g+s /shared/sysadm
su -harry
cd /shared/sysadm
ll touch harry-file
##########################################################################################################################
 useradd ram
    2  cat /etc/passwd
    3  passwd ram
    4  groupadd sales
    5  groupadd marketing
    6  ll
    7  cat /etc/passwd
    8  usermod -a -G sales marketing ram
    9  cat /etc/passwd
   10  usermod -a -G sales,marketing ram
   11  cat /etc/passwd
   12  cat /etc/group
   13  history
##########################################################################################################################
   useradd sanjay
   passwd sanjay
   vim /etc/sudoers
whereis useradd 
whereis passwd
   user1 = add ....useradd user2 , .....passwd user2
   sri     ALL=(ALL)       /usr/sbin/useradd pooja
   
   sudo su sanjay

   useradd pooja
   passwd pooja

##########################################################################################################################
TOMCAT-SERVER
* first 3 instances dev-server , jenkins-server , web-server , tomcat-server
dev-server = t2.micro , size = 12
jenkins-server = t2.medium , size = 8
tomcat-server = t2,micro , size = 10
ON DEV-SERVER 
sudo su -
set hostname - dev-server.example.com
bash
yum install git 
CREATE A PUBLIC REPO AND CLONE
 1  yum install git
    2  history
    3  git clone git@github.com:sanjayguruji/web-repo.git
    4  git clone https://github.com/sanjayguruji/web-repo.git
    5  ssh-keygen
    6  cd .ssh/
    7  cat id_rsa.pub (add the ssh key to the github)
    8  cd
    9  cd web-repo
   10  git init
   11  git add .
   12  git commit -m devops -a
   13  git branch -M main
   14  git remote add origin git@github.com:mansi-lti/tomcat.git
   15  git remote remove origin
   16  git remote add origin git@github.com:mansi-lti/tomcat.git
   17  git push origin main
CONNECT TO JENKINS SERVER
sudo su -
hostname
bash
 wget -O /etc/yum.repos.d/jenkins.repo     https://pkg.jenkins.io/redhat-stable/jenkins.repo
    2  rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key
    3  yum upgrade
    4  yum install java-17-amazon-corretto -y
    5  yum install jenkins -y
    6  systemctl enable jenkins
    7  systemctl start jenkins
    8  systemctl status jenkins
go to ithe browser and copy instance ip-address:8080 and create webhook
after sign in install plugins like maven , deploy to container , github integration then restart the jenkins
yum install maven
CONNECT TO APACHE SERVER
sudo su -
hostname = apache.example.com
bash
yum install java*
from google copy the link for tomcat 9 download select tar.gz file
 yum install wget
 wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.106/bin/apache-tomcat-9.0.106.tar.gz
 tar -xzf apache-tomcat-9.0.106.tar.gz
 ll
move to jenkins server 
mvn -v (copy the maven home path and java path and paste in the jenkins web server under tools)
next create new item and start the build if not coming install git on jenkins server
next move to apache server 
 cd  apache-tomcat-9.0.106
 cd bin
  ll
  chmod +x startup.sh
  chmod =x shutdown.sh
  chmod +x shutdown.sh
open the medium and follow the steps
after runnig the ./startup copy the ip address with port no 8080
login manager app username and pass admin 
then the configuration part and create credentials with username depolyer and pass depolyer
add container
make changes from github and check wheather the changes are reflecting or not
####################################################################################################
Docker_installation & Containers
:: yum install docker*

:: docker pull jenkins/jenkins

:: docker ps

:: systemctl start docker

:: systemctl enable docker

:: docker info

:: docker ps

:: Creating a container for jenkins
-- docker run -itd --name <Name_of_container> -p 8080:8080 jenkins/jenkins

:: docker ps -a

:: docker exec -it <container_id> /bin/bash

:: after we logged into 
-- <public ip-address>:8080
-- copy the directoryy

:: cat <paste_directory>

:: copy the password and paste it in browser login page
###################################################################################################
jenkins_on_aws
:: up to date on your instance by using
--- yum update –y

:: Add the Jenkins repo using the following command
--- wget -O /etc/yum.repos.d/jenkins.repo \
    https://pkg.jenkins.io/redhat-stable/jenkins.repo

:: Import a key file from Jenkins-CI to enable installation from the package
--- rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key
--- yum upgrade

:: Install Java
--- yum install java-17-amazon-corretto -y

:: Install Jenkins
--- yum install jenkins -y

:: Enable the Jenkins service to start at boot
--- systemctl enable jenkins

:: Start Jenkins as a service
--- systemctl start jenkins


:: You can check the status of the Jenkins service using the command
--- systemctl status jenkins
 ............
 ###################3
 ################
 #################
 ###########
 :: yum install docker*

:: docker pull jenkins/jenkins

:: docker ps

:: systemctl start docker

:: systemctl enable docker

:: docker info

:: docker ps

:: Creating a container for jenkins
-- docker run -itd --name <Name_of_container> -p 8080:8080 jenkins/jenkins

:: docker ps -a

:: docker exec -it <container_id> /bin/bash

:: after we logged into 
-- <public ip-address>:8080
-- copy the directoryy

:: cat <paste_directory>

:: copy the password and paste it in browser login page
 ###########
 #########33
 ##########3
 ##########
 :: up to date on your instance by using
--- yum update –y

:: Add the Jenkins repo using the following command
--- wget -O /etc/yum.repos.d/jenkins.repo \
    https://pkg.jenkins.io/redhat-stable/jenkins.repo

:: Import a key file from Jenkins-CI to enable installation from the package
--- rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key
--- yum upgrade

:: Install Java
--- yum install java-17-amazon-corretto -y

:: Install Jenkins
--- yum install jenkins -y

:: Enable the Jenkins service to start at boot
--- systemctl enable jenkins

:: Start Jenkins as a service
--- systemctl start jenkins


:: You can check the status of the Jenkins service using the command
--- systemctl status jenkins



 ############3
 ##########
 ##########33
 #########



 git 

 git ::
-------------
Server:

    1  yum install git -y

    2  ip a s

    3  hostname

    4  vim /etc/hosts

    5  cat vim /etc/hosts

    6  ping client-one.example.com

    7  ping client-two.example.com

    8  mkdir project

    9  cd project

   10  ls

   11  ls -a

   12  git init --bare

   13  ls -a

   14  cd

   15  passwd root

   16  vim /etc/ssh/sshd_config

   17  systemctl restart sshd

   18  systemctl enable sshd

   19  hostname

   20  cd project

   21  cd branches

   22  ls

   23  cd

   24  cd project

   25  ls -a

   26  cd branches

   27  ls -a

   28  cd ..

   29  ll

   30  ls -a

   31  cd branches/

   32  ls

   33  cd
 
----------------------------------------------------------------------------------------------------------------
 
Client-1:

    1  yum install git -y

    2  ip a s

    3  hostname

    4  vim /etc/hosts

    5  cat vim/etc/hosts

    6  cat /etc/hosts

    7  ping git-server.example.com

    8  ping client-two.example.com

    9  mkdir git-one

   10  cd git-one

   11  git init

   12  ls -a

   13  cd

   14  ssh-keygen

   15  cd .ssh/

   16  ls -a

   17  ssh-copy-id root@git-server.example.com

   18  cd

   19  cd git-one

   20  ll

   21  ls -a

   22  vim index.html

   23  git add index.html

   24  git commit -m "first" index.html

   25  git status

   26  git remote add origin root@git-server.example.com:/project

   27  git remote remove origin

   28  git remote add origin root@git-server.example.com:/project

   29  git push origin master

   30  git remote remove origin

   31  git remote add origin root@git-server.example.com:project

   32  git push origin master
 
------------------------------------------------------------------------------------------------------------------

Client-2:

    1  cd git-two

    2  git remote add origin root@git-server.example.com:project

    3  git pull origin master

    4  cat index.html



 ###################################################################################################################################################
 EFS (FOR CREATING A FILE IN AWS AND TRYING  TO GET IN REDHAT AND UBUNTU IN DIFFERENT 1A,1B,1C)

AWS
  1  yum install nfs-utils -y
    2  yum install
    3  yum install nfs-utils -y
    4  systemctl restart nfs
    5  systemctl start nfs
    6  systemctl start nfs-server.service
    7  systemctl enable nfs-server.service
    8  systemctl start nfs-server.service
    9  systemctl enable nfs-server.service
   10  systemctl status nfs-server.service
   11  mkdir efs
   12  sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 172.31.34.186:/ efs
   13  df -h
   14  cd /root/efs
   15  touch devops{1..100}
   16  ll
   17  history
   18  touch cloud{1..10}
   19  ll
   20  mkdir uday
   21  ls
   22  ]cd
   23  cd
   24  cd efs
   25  mkdir yhk
   26  ls
   27  history

 REDHAT
  1  yum install nfs-utils -y
    2  systemctl start nfs-server.service
    3  systemctl enable nfs-server.service
    4  mkdir remote
    5  sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 172.31.34.186:/ efs
    6  sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 172.31.34.186:/ remote
    7  df -h
    8  cd /root/remote
    9  ls
   10  history
   11  ll
   12  ls
   13  cd
   14  ls
   15  cd r
   16  cd remote/
   17  ls
   18  history

 UBUNTU
     1  cat /etc/os-release
    2  apt-get update -y
    3  apt install nfs-common
    4  systemctl status nfs-
    5  syustem start nfs-common.service
    6  system start nfs-common.service
    7  system unmask nfs-common.service
    8  system start nfs-common.service
    9  system enable nfs-common.service
   10  systemctl status nfs-
   11  systemctl status nfs-common.service
   12  system enable nfs-common.service
   13  system start nfs-common.service
   14  system enable nfs-common.service
   15  history
   16  mkdir devops
   17  sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 172.31.34.186:/ efs
   18  sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 172.31.34.186:/ devops
   19  cd devops
   20  ls
   21  df -h
   22  ll
   23  history
   24  ll
   25  history

 TRYING TO CREATE IN 1C AN STOPPING 
 (TRAIL :
  1  mkdir srija
    2  sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 172.31.34.186:/ srija
    3  ll
    4  cd /srija
    5  cd srija
    6  ll
    7  cd srija
    8  ll
    9  ls
   10  rmdir uday
   11  cd
   12  history)
 #############################################################################################################################
  yum update -y
    2  yum install automake fuse fuse-devel gcc-c++ git libcurl-devel libxml2-devel make openssl-devel
    3  git clone https://github.com/s3fs-fuse/s3fs-fuse.git
    4  ll
    5  curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
    6  unzip awscliv2.zip
    7  sudo ./aws/install
    8  aws --version
    9  aws configure
   10  ll
   11  cd .aws/
   12  ls
   13  cat config
   14  cat credentials
   15    cd
   16  sudo apt-get install automake autotools-dev fuse g++ git libcurl4-gnutls-dev libfuse-dev libssl-dev libxml2-dev make pkg-config
   17  apt-get install automake autotools-dev fuse g++ git libcurl4-gnutls-dev libfuse-dev libssl-dev libxml2-dev make pkg-config
   18  yum install automake fuse fuse-devel gcc-c++ git libcurl-devel libxml2-devel make openssl-devel
   19  git clone https://github.com/s3fs-fuse/s3fs-fuse.git
   20  ll
   21  /autogen.sh
   22  cd s3fs-fuse
   23  /autogen.sh
   24  cd s3fs-fuse/
   25  ./autogen.sh
   26  ./configure --prefix=/usr --with-openssl
   27  make
   28  make install
   29  which s3fs
   30  touch /etc/passwd-s3fs
   31  vim /etc/passwd-s3fs
   32  chmod 640 /etc/passwd-s3fs
   33  cd
   34  s3fs sriamju1234 /mnt -o passwd_file=/etc/passwd-s3fs
   35  df -h
   36  cd /mnt
   37  ll
   38  touch dev.txt{1..5}
   39  history
##########################################################################################################################
  ##########################################################################################################################
 creating own vpc

1 :-vpc :-  vpc only - name(devops-vpc) -ip(10.0.0.0/16) -create
2 :- internet gate way :- internet gateway - default(name it - default-iwg) -  create(devops-igw) - actions - attach to vpc
3 :-subnet - create - name(public-subnet) - zone(us-eat 1a) - (10.0.0.0/16) -(10.0.0.0/24)
             create - name(private-subnet) - zone(us-eat 1b) - (10.0.0.0/16) -(10.0.1.0/25)
4 :- route table :- route table - create - name(public-rt) - edit routs - 0.0000/00 - internet... 
                                                                                    - select yours...
                    click on public-rt - actions -edit subnet associations - select public
 5 :- instances :- ec2 - create - name(public-inst) - auto assign(enable) - vpc(select urs) - subnet (us east 1a) - security group (create new)
                   allow access to (http , icmp) -launch
                   ec2 - create - name(private-inst) - auto assign(disable) - vpc(select urs) - subnet (us east 1b) - security group (create new)
                   allow access to ( icmp) -launch

 
 open terminal of public-instance 
:- sudo su -
    yum install httpd -y 
    cd /var/www/html/
    echo "this is web server" >> index.html
    systemctl restart httpd
    systemctl enable httpd
    cd 
    ping google.com
   
 
 6 :- NAT :- crete - name(devops-nat) - subnet(public) -click allocate and create 
 7 :- route table :- route table - create - name(private-rt) - edit routs - 0.0000/00 - NAT... 
                                                                                    - select yours...
                    click on private-rt - actions - edit subnet associations - select private
      
 in terminal :-
     vim lti-mahape-key.pem  - now open the key in the system and paste the key 
    chmod 400 lti-mahape-key.pem
    ssh -i lti-mahape-key.pem ( open private-instance - connect -ssh  - in the url ther is (ec2-user@..) copy and pastee)
    ping google .com

 #########################################################################################################33
 #######################################################################################################

 To mount bucket and files 

 1 :- iam - users -create - name(sriii) - permissions - attach policy(amazons3full) 
      click on user - security credential - cli - tick on i understand - create - download csv file - done
  connect  to terminal 
      sudo su -
      1  yum update -y
    2  yum install automake fuse fuse-devel gcc-c++ git libcurl-devel libxml2-devel make openssl-devel
    3  git clone https://github.com/s3fs-fuse/s3fs-fuse.git
    4  ll
    5  curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
    6  unzip awscliv2.zip
    7  sudo ./aws/install
    8  aws --version
    9  aws configure
   10  ll
   11  cd .aws/
   12  ls
   13  cat config        give access key , secret key , tab
   14  cat credentials
   15    cd
   18  yum install automake fuse fuse-devel gcc-c++ git libcurl-devel libxml2-devel make openssl-devel
   19  git clone https://github.com/s3fs-fuse/s3fs-fuse.git
   20  ll
   21
   24  cd s3fs-fuse/
   25  ./autogen.sh
   26  ./configure --prefix=/usr --with-openssl
   27  make
   28  make install
   29  which s3fs
   30  touch /etc/passwd-s3fs
   31  vim /etc/passwd-s3fs    give(access key : secret key)
   32  chmod 640 /etc/passwd-s3fs
   33  cd
   34  s3fs name_of_the_bucket_where_files_are_present /mnt -o passwd_file=/etc/passwd-s3fs
   35  df -h
   36  cd /mnt
   37  ll
   38  touch dev.txt{1..5}
 everything will be mounted
 
#############################################################################################################################
 ###########################################################################################################################

 to make elastic ip
 1:-  instance - create - allow (http ) - security group (new) - storage(10 gib)- deletion on termination(yes)
 2:- open terminal
    # sudo su -
     yum install httpd -y
     rpmquery httpd
     cd /var/www/httpd
     cat > index.html
     systemctl start httpd 
     systemctl enable httpd
     systemctl status httpd 
 3:- copy the ip address and paste it will show the content 
 4:- reboot instance - and copy the ip address and paste it will show the content(ip will be changet)
 5:- elastic ip - create - click on allocate
     click on eip - actions - associate eip to instance - giver instance and private ip - click assosiate
 ###################################################################################################################################
 #################################################################################################################################

 To prevent instancee from termination and stopping
 1:-  instance - create(allow http)
  2:- open terminal
    # sudo su -
     yum install httpd -y
     rpmquery httpd
     cd /var/www/httpd
     cat > index.html
     systemctl start httpd 
     systemctl enable httpd
     systemctl status httpd 
 3:- click on the instance - actions - instance settings - click on stop protection / termination protection

 ###########################################################################################################################
 ##########################################################################################################################
 to make things happen on its own
 1:- instance - allow(http) -advance - type 
 #!/bin/bash
 yum install httpd -y
 echo "this is derver" >> /var/www/html/
 systemctl start httpd
 systemctl enable httpd
 useradd srija -p "redhat" -c "comment"

 2:- copy the ip and paste in the url
 3:-open terminal
 sudo su -
 cd /var/www/html
 cat index.html  (file is present with content )
 tail /etc/passwd
 tail /etc/shadow

 ############################################################################################################################3
 ##########################################################################################################################

 to create own templet
 1:-create templet - storage(ebs 10)
 2:- click on templet - actions - create instance from templet - chooser source - choose number of instances and launch 

 #########################################################################################################################
 ########################################################################################################################

[n.virginia] 1:-instance - create
             2:-volume - name existing one - click on that and click on create new volume - name - size (5) -save 
                click on new voulume - action s- attach - give instance and device - save
             3:- open terminal :- sudo su -
                                    mkdir /data
    2  lsblk
    3  mount /dev/xvdb /data
    4  mkfs.ext4 /dev/sdb
    5  mount /dev/xvdb /data
    6  cd /data
    7  touch dev.txt{1..5}
    8  ll
    9  cd
            4:- snapshots - create - volume - select new data volume 
                click on snapshot - action - copy - destnination -us-east-2 - copy snapshot
 [ohio]  - 1:- create new instance 
            2:- snapshot - click on snapshot -action - create new volume from shap shot
               - click on new volume - action -attch to instance - ...
           3:- open terminal
             sudo su -
                lsblk
             mkdir /data
              mount /dev/xvdb /data
             cd /data
             ll
               cd


 ############################################################################################################################
 #######################################################################################################################3
  EFS
 1:- instance - first - aws - us east 1a
                second - redhat -us east 1b
                third - ubuntu -us east 1a - enable 2049 nfs
 2 :- efs - create 
 3:- connect 
     terminal one -
      sudo su -
     yum install nfs-utils -y
    2  systemctl restart nfs-server.sevice
    3  systemctl enable nfs-server.service
     terminal two 
     sudo su -
      sytemctl start nfs-server.service
    2  systemctl enable nfs-server.service
     terminal three
     sudo su -
      systemctl status nfs-common.service
    3  systemctl start nfs-common.service
    4  systemctl unmask nfs-common.service
    5  systemctl start nfs-common.service
    6  systemctl enable nfs-common.service
    7  systemctl status nfs-common.service

 4:- security group - create(efs-sg) - desc - invound(add rule nfs set value ) - create
 5:- efs -filesystem -network mange - select efs-sg for 1a and 1b) - create
     attach - mount ip -copy the url 
 6:- terminal
     one -mkdir efs
    5  sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 172.31.41.201:/ efs
    6  df -h
    7  cd /root/efs
    8  touch dev.txt{1..5}
    9  ll
   10  cd
    two 
      mkdir remote
    4  sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 172.31.41.201:/ remote
    5  df -h
    6  cd /remote
    7  cd remote
    8  ll
    9  cd
   three
     mkdir rem
   10  sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 172.31.41.201:/ rem
   11  cd rem
   12  ll
 #############################################################33333333####################################################
 #######################################################################################################################

 Tomcat , jenkins and github 
 1:- instance(dev-server) - 
 2:-  connect the instance 
    in terminal :-  
     sudo su -
     ssh-keygen
   
    3  cd .ssh/
    4  ll
    6  cat id_rsa.pub
    7  yum install git -y
    8  cd
   3:-  open github - create a new repo 
     settings - ssh key - new ssh -paste the content of the key
   4:- git clone https://github.com/sanjayguruji/web-repo.git
   
   19  cd web-repo
   20  git init
   21  git add .
   22  git commit -m "first commit"
   23  git branch -M main
   24  git remote -v
   25  git remote remove origin
   26  git remote add origin git@github.com:AMJURI329/new.git
   27  git remote -v
   28  git push -u origin main
   29  cd
 5:- check in the github whether u got the details or not 
 6:- create a jenkins instance - medium - storage(15)
     connect terminal -
      sudo su -
       yum update –y
    2  wget -O /etc/yum.repos.d/jenkins.repo     https://pkg.jenkins.io/redhat-stable/jenkins.repo
    3  rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key
    4  yum upgrade
    5  yum install java-17-amazon-corretto -y
    6  yum install jenkins -y
    7  systemctl enable jenkins
    8  systemctl start jenkins
    9  systemctl status jenkins
    7:- now copy ip address of jenkins instance and paste:8080
    copy the url and in terminal : cat ...url...
    paste the passwd
    install plugins- username ...details...
    8:  github - settings -webhooks - new webhook -ipaddres of jenkins(/github-webhook/)- json -secret(jenkin - profile -security -add new token -)
     9:- jenkin dashboard manage jenkins - plugin - additional plugin -maven ,deploy to coontainer ,github integration install 
     in terminal jenkins -yum install maven 
                          yum install git -y
                          mvn -v (copy this)
    
    10:- jenkins  - new item -maven -create - git - */main
          jenkins - tools - java - maven 
         
   11:- create apache instance -medium - connect - 
        in terminal - sudo su -hostnamectl set-hostname tomcat.com - bash - yum install java* -  yum install wget -  wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.105/bin/apache-tomcat-9.0.105.tar.gz -
           tar -xvzf apache-tomcat-9.0.105.tar.gz - ll - cd  apache-tomcat-9.0.105 - ll -  chmod +x startup.sh - chmod +x shutdown.sh - cd .. - cd conf/ - ll - vim tomcat-users.xml - cd .. - vi ./webapps/examples/META-INF/context.xml - vi ./webapps/host-manager/META-INF/context.xml -
           vi ./webapps/manager/META-INF/context.xml - ll - cd bin/ - ./startup.sh
   12 - now open apache - copy ip of tomcat : 8080 - managet app - admin admin 
   13:- in jenkins - configure click postbuild - deploy war / ear - war (**/*.war) - content path(/) - add container (tomcat 9) - 
      url(tomcact url 8080) -credentials selcect 
     14: manage jenkns - credential - global - deployer deployer 
   for auromation :- configure - github hook trigger 
 ################################################################################################################################
 ##################################################################################################################################

 ******Using 2 vpc sending data from one region to another*******
N.Virgina
Create vpc :- dev-vpc , 10.0.0.0/16
Igw :- dev-igw , attach igw to vpc
subnet :- public-subnet , 1a , 10.0.0.0/16 , 10.0.0.0/24
          private-subnet , 1b , 10.0.0.0/16, 10.0.1.0/24
route table :- public-rt , dev-vpc
edit routes :- 0.0.0-- , igw , save changes
route table :- private-rt , dev-vpc 
associate subnets
create instance :- public-server , lti keuy , dev-vpc , public-subnet , create sg , add http , icmp
create instance :- private-server , dev-vpc , private-subnet , disabled , existing sg
connect 
#sudo su -
#yum install httpd -y
#cd /var/www/html
#echo "this is web server" > index.html
#ll
#systemctl restart httpd
#systemctl enable httpd
paste public ip of ng instance
#cd
copy private-server ip add
#ping ip_add
----------ohio-region----------
create vpc :- AI-project , 20.0.0.0/16
igw - AI-project-igw , attach to vpc 
subnet :- web-subnet , 2a , 20.0.0.0/24
          db-subnet , 2b, 20.0.1.0/24
route table :- web-rt , AI-project,   
edit routes :- igw
associate subnets
route table :- db-rt 
instances :- create :- web-server , sg(web-sg)(new) , allow icmp , http
create : db-server 
connect
#sudo su -
#yum install httpd -y
#ping ip_add (private ip of db_Server)
#passwd root 
#vim /etc/ssh/sshd_config (edit permintrootpassword , passwordauthentication -> yes)
systemctl restart sshd
systemctl enable sshd
do these changes in the both machines
create files on the both the machines 
cat > devops.txt provide some content in that (similary do it in N.virgina region)
create peering connections 
name : dev-to-ai
vpc : dev-pc
another region 
us-east-2
vcpid (copy vpc id from the ohio region)
in ohio accept request -> under peering connections 
--------ohio-region----------------
route table :- web-rt 
edit :- add (10.0.0.0/16)  peering connection
--------N.V------------
route table :- private
edit :- add(20.0.0.0/16) peering connection 
------ohio terminal----------
scp devops.txt root@public-ip-(nv-region-termial):/tmp
------nv terminal----------
cd /tmp
ll
vice-versa (optional)
=====================================================================================================
MFA 
first login to aws account -> security credentials -> mfa device
give mfa device name -> scane using authentictor app -> provide two codes that comes in authenticator app 
you can also disable it uh will see a option to remove 
========================

 #######################################################################################################################################################################
 ###################################################################################################################################################################

 DOCKER

 launch instance - allow ports on which containers are going to run (i.e, 8080,8081,8082)
 #yum install docker*
   systemctl enable docker
  systemctl start docker
  docker info 
 docker ps -a
 docker image ls
 docker --help
 docker pull ubuntu
 docker pull httpd 
 docker run -it --name web-app -p 8080:8080 ubuntu /bin/bash
    apt update -y
    apt install apache2 -y
    cd /var/www/html
    echo "this is web" > index.html
    service apache2 start
    (ctrl p ctrl q ) to come out
 curl http://(ip of the instance)
 docker inspect web-app | less     #(get ip from here)
 
  docker run -it --name web-app1 -p 8081:8080 ubuntu /bin/bash
    apt update -y
    apt install apache2 -y
    cd /var/www/html
    echo "this is web1" > index.html
    service apache2 start
    (ctrl p ctrl q ) to come out
 curl http://(ip of the instance)

 
 
 docker stop web-app
 docker rm web-app

 docker kill web-app
 docker rm web-app


 creating volume 
  docker volume create my-vol
   docker volume ls
   docker run -it -v my-vol:/var/www/html -p 8083:80 --name vol1 ubuntu /bin/bash
      apt update -y
     apt install apache2 -y  
    cd /var/www/html
       echo "this is vol" > index.html
       cd
     service apache2 start
     ctrlp + ctrl Q
      cd /var/lib/docker/volumes
      ll
       docker stop vol1
     docker rm vol1
     docker run -it -v my-vol:/var/www/html --name vol2 ubuntu /bin/bash
     cd /var/www/html
      ll
     cat index.html


 files ---------
  mkdir /data
  docker run -it -v /data/:/tmp --name copying ubuntu /bin/bash
  cd /tmp
   touch data1.txt{1..5}
   cd
   ctrlP+ctrlq
   cd /data
   ll
 (all files will be visible )


 -----------------------------------------
 docker run -it --name web -p 8080:80 nginx /bin/bash 
 apt update -y
  apt install nginx
 service nginx status
 service nginx start
 service nginx status
 ctrlp + curlq
  curl http://172.17.0.2
 in google ip:8080
 ---------------------------

 docker run -d --name web7 -p 8082:80 nginx
     docker exec -it web7 bash
  cd /usr/share/nginx/html
    2  cat > index.html
 ip:8082
 


 
#################################################################################################################################################33333333################333
 ########################################################################################################################################################################

 ANSIBLE

---common ansible commands usage----
 controller 
      vim /etc/hosts
      passwd root
      ssh-keygen
      yum install ansible*
       cd /etc/ansible
       vim hosts  : [dev-server]
                    one.com
                     [prod-server]
                     two.com
 one.com   
      vim /etc/hosts
      passwd root
      vim /etc/ssh/sshd_config
      systemctl start sshd 
       systemctl enable sshd
       vim authorized_keys

 two.com

      vim /etc/hosts
      passwd root
      vim /etc/ssh/sshd_config
      systemctl start sshd 
       systemctl enable sshd
       vim authorized_keys
 
        
create 3 instances - ansible-controller , ansible node1 , ansible node2 -> connect , set hostname 
put the entry of ip and hostname in /etc/hosts file
generate ssh key on controller and copy it under authorized keys in node1 and node 2
install anisble only on controller
#cd /etc/ansible
#vim hosts
[dev-servers]
worker1-server
[prod-server]
worker2-server
*ping command :- ansible all -m ping
*ping with desired output :- ansible all -m ping -a data=hello
*to add user :- ansible all -m user -a 'name=mansi uid=1200 home=/ltimindtree state=present'
check in target machine using :- cat /etc/passwd | grep -i mansi
to create a file :- ansible all -m file -a 'path=/tmp/mansi.txt state=touch'
to create a directory :- ansible all -m file -a 'path=/tmp/mydirectory state=directory' 
to pull something from the target to controller:- ansible worker1-server -m fetch -a 'src=/tmp/mansi.txt dest=/mnt'
to copy:- ansible all -m copy -a 'src=/etc/fstab dest=/tmp'
to see disk info :- ansible all -m command -a 'df -hT'
to see partition info :- ansible all -m command -a 'fdisk -l'
                        ansible all -m command -a 'cat /etc/passwd'


----working with playbook-----------
#cd /etc/ansible
#mkdir project-x
vim sample-playbook.yaml
---
- name: this is sample playbook
  hosts: all
  tasks:
    - name: creating user
      user:
        name: sachin
        uid: 1300
        home: /heros
        shell: /bin/bash
        state: present
    - name: installing apache
      yum:
        name: httpd
        state: present
    - name: install cifs
      yum:
        name: cifs-utils
        state: present
    - name: install vsftpd
      dnf:
        name: vsftpd
        state: latest
...
# ansible-playbook sample-playbook.yaml --syntax-check
#ansible-playbook sample-playbook.yaml -C
#ansible-playbook sample-playbook.yaml
verification on target
rpmquery httpd
   31  rpmquery vsftpd
   32  rpmquery cifs-utils
   33  cat /etc/passwd | grep -i sachin

----configuring basic apache server ---------
create 3 instances 
and provide connection between it by making the entry of hostname and ip in /etc/hosts file
generate ssh key and copy to the target machine under authorized keys
install ansible on controller machine
make the entry in /etc/ansible
[dev-servers]
ansible-target
[prod-servers]
ansible-target2
#cd /etc/ansible 
#vim basic-apache.yaml
---
- name: configure basic apache
  hosts: all
  tasks:
    - name: installed httpd
      dnf:
        name: httpd
        state: latest
    - name: copied index.html
      copy:
        src: index.html
        dest: /var/www/html/index.html
    - name: started httpd
      systemd:
        name: httpd
        state: started
        enabled: true
 #ansible-playbook basic-apache.yaml -C
 #ansible-playbook basic-apache.yaml
verify in target machines

-----------handler section------------------------------------
remove the httpd package if installed on the 3 machines
install on controll machine
 yum install httpd -y
   48  cd /etc/httpd/conf
   49  cp httpd.conf /etc/ansible/project-x
   50  cd
make a project-x directory in ansible 
#cd /etc/httpd/conf
#cp httpd.conf /etc/ansible/project-x
#cd
make changes in the httpd.conf file
#vim hanlders.yaml
---
- name: using handlers with apache
  hosts: ansible-target
  tasks:

    - name: installed httpd
      dnf:
        name: httpd
        state: latest

    - name: copied httpd.conf file
      copy:
        src: httpd.conf
        dest: /etc/httpd/conf/httpd.conf
      notify: restart_httpd

    - name: copied index.html
      copy:
        src: index.html
        dest: /var/www/html/index.html

    - name: started httpd
      systemd:
        name: httpd
        state: started
        enabled: true

    - name: installed firewalld
      dnf:
        name: firewalld
        state: present
    - name: started firewalld
      systemd:
        name: firewalld
        state: started
    - name: add port 81 in firewall
      firewalld:
        port: 81/tcp
        permanent: yes
        state: enabled
        immediate: true
  handlers:
    - name: restart_httpd
      service:
        name: httpd
        state: restarted

--------ec2 provisioning via ansible-----------------
create an instance
#sudo su -
#yum install ansible* -y
#mkdir -p /etc/ansible
#cd /etc/ansible
#vim ansible.cfg
#ansible-galaxy collection install amazon.aws
#cd
#yum install python* --allowerasing --skip-broken
#yum install awscli --user --upgrade
#pip install boto
 #pip install botocore
#dig google.com
#cd /etc/ansible
#ll
#vim hosts
[localhost]
localhost
#ssh-keygen
#aws configure
 #passwd root
 #vim /etc/ssh/sshd_config
 #systemctl restart sshd
  #systemctl enable sshd
  #cd .ssh/
  #ssh-copy-id root@localhost
  #cd
  #cd /etc/ansible


---
- hosts:  localhost
  tasks:
    - name: creating Ec2 instance via Ansible
      amazon.aws.ec2_instance:
        aws_access_key: 
        aws_secret_key: 
        name: K8s-Master
        instance_type: t2.medium
        image_id: ami-0a7d80731ae1b2435
        vpc_subnet_id: subnet-081c9e4ce171661d5
        key_name: lti-mahape-key
        region: us-east-1
        count: 1
        state: present
 
#pip install botocore
#vim hosts
[localhost]
localhost
localhost ansible_connection=local ansible_python_interpreter=/usr/bin/python3 

#ansible-playbook ec2.yaml --syntax-check
#ansible-playbook ec2.yaml -C
#ansible-playbook ec2.yaml 




 controller
     vim /etc/hosts
    3  ping node1.com
    4  pine node2.com
    5  ping node2.com
    6  yum install ansible*
    7  passwd root
    8  ssh-keygen
    9  cd .ssh/
   10  ll
   11  cat id_rsa.pub
   12  cd
 cd /etc/ansible
   18  vim hosts

     [dev-servers]
     node1.com
     [prod-servers]
     node2.com
   19  vim simple-playbook.yaml

       ---
       - name: this is playbook
         hosts: all
         tasks:
           - name: install apache
             yum:
                name: httpd
                state: present
          - name: started apache
            systemd:
                name: httpd
                state: started
                enabled: true
       
   20  ansible-playbook simple-playbook.yaml --syntax-check
   21  ansible-playbook sample-playbook.yaml
   22  ansible-playbook simple-playbook.yaml

 node1.com
      vim /etc/hosts
    3  cd .ssh
    4  ll
    5  vim authorized_keys
    6  cd
    7  rpmquery apache
    8  rpmquery httpd
    9  systemctl status httpd

 node2.com
     vim /etc/hosts
    2  cd .ssh/
    3  vim authorized_keys
    4  cd
    5  history


 #######################################################################################################################################################################
###########################################################################################################################################################################
 
 
KUBERNETES

 kops----------------------------------------------------------------------------------------------------------------------------------------------------------------

 ----Cluster creating using kops--------
Create an instance :- ubuntu -22.04 , t2.medium , size:-15 . sg :- kube-sg
-Install awscli
  apt install unzip -y
  curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
  unzip awscliv2.zip
  sudo ./aws/install
  aws configure
-Install kubectl
  curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
  sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
  kubectl version --client
-Install kops
  curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64
  chmod +x ./kops
  sudo mv ./kops /usr/local/bin/
  kops version
Create an IAM user/role with Route53, EC2, IAM and S3 full access
Attach IAM role to ubuntu server :- Instance -> actions -> security -> modify iam -> attach role
Create a Route53 private hosted zone :- route53 -> create hosted zone 
Domanin Name :- some_name.in
Type :- private hosted zone
Region :- us-east-1
vpc id :- default
Create hosted zone
Create s3 bucket using command line
  aws s3 mb s3://dev.k8s.mansi-kops-2610.sreeja.in [change name before creating:- name should be like bucket_name.dns_name(route53 zone)]
To check 
  aws s3 ls
  export KOPS_STATE_STORE=s3://dev.k8s.mansi-kops-2610.sreeja.in
generate ssh-key:-
  ssh-keygen
create cluster                                                  
  kops create cluster --cloud=aws --zones=us-east-1a --name=dev.k8s.mansi-kops-2610.sreeja.in --dns-zone=sreeja.in --dns private
  kops update cluster --name dev.k8s.mansi-kops-2610.sreeja.in --yes --admin
  kops validate cluster  (it takes some time to be active dont worry)
  kubectl get nodes
----Deploying Nginx container on Kubernetes--------
 kubectl create deployment sample-nginx --image=nginx
   kubectl scale deployment sample-nginx --replicas=2
   kubectl get pods
   kubectl expose deployment sample-nginx --port=80 --target-port=80 --type=LoadBalancer
   kubectl get svc
-----Deleting cluster ---------
kops delete cluster dev.k8s.sanjayskv.in --yes
 
Host EKS CLUSTER via EKSCTL-----------------------------------------------------------------------------------------------------------------------------------------------

 create iam role and attach policy
 ECRfull access,EKSpolicy and IAM full access
 apt-get update -y
 apt install unzip -y
 curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
 unzip awscliv2.zip
 sudo ./aws/install
 aws configure
 Install EKS Tool
 curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
 sudo mv /tmp/eksctl /usr/local/bin
 eksctl version
 Install Kubectl
 curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
 sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl 
 kubectl version --client
 SSh-keygen
 Create EKS Cluster
 eksctl create cluster --name my-cluster --region region-code --version 1.32 --vpc-public-subnets subnet-ExampleID1,subnet-ExampleID2 --without-nodegroup

 ##Create a Node Group##

 eksctl create nodegroup \
  --cluster my-cluster \
  --region us-east-2 \
  --name my-node-group \
  --node-ami-family Ubuntu2204 \
  --node-type t2.small \
  --subnet-ids subnet-086ced1a84c94a342,subnet-01695faa5e0e61d97 \
  --nodes 3 \
  --nodes-min 2 \
  --nodes-max 4 \
  --ssh-access \
  --ssh-public-key /root/.ssh/id_rsa.pub

or you can make cluster in this way

 eksctl create cluster --name milestone-2 --region us-east-1 --version 1.32 --node-type t2.small --nodes 3 --nodes-min 2 --nodes-max 4 --ssh-access --ssh-public-key /root/.ssh/id_rsa.pub




 When You want to delete cluster
 eksctl delete cluster --name my-cluster

 --------------------------------------------------------------------------------------------------------------------------------------------------------------
 pod.yaml
 
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
 ------------------------------------------------------------------------------------------------------------------------------------------------------------
 replica.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # modify replicas according to your case
  replicas: 10
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5
 -----------------------------------------------------------------------------------------------------------------------------------------------

 deployment.yaml
 apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 6
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.16.1
        ports:
        - containerPort: 80
 ------------------------------------------------------------------------------------------------------------------------------
 ------garenteed------
 apiVersion: v1
kind: Pod
metadata:
  name: sanjaya-app
  namespace: default
  labels:
   app: sanjaya-app
spec:
  containers:
    - name: web-app
      image: nginx
      resources:
        limits:
          memory: "250Mi"
          cpu: "400m"
        requests:
          memory: "250Mi"
          cpu: "400m"

 ----------brustable----------
 apiVersion: v1
kind: Pod
metadata:
  name: sanjay-app
  namespace: default
  labels:
   app: sanjay-app
spec:
  containers:
    - name: web-app
      image: nginx
      resources:
        limits:
          memory: "250Mi"
        requests:
          memory: "150Mi"

 ------------best effort----------
apiVersion: v1
kind: Pod
metadata:
  name: sanjaya-app1
  namespace: default
  labels:
   app: sanjaya-app1
spec:
  containers:
    - name: web-app
      image: nginx

 
---------------------------------------------------------------------------------------------------------------------------------

  apt install unzip -y
    2  curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
    3  unzip awscliv2.zip
    4  sudo ./aws/install
    5  aws configure
    6  eksctl version
    7  ssh-keygen
    8  eksctl create cluster --name my-cluster --region region-code --version 1.32 --vpc-public-subnets subnet-ExampleID1,subnet-ExampleID2 --without-nodegroup
    9  eksctl create cluster --name my-cluster --region us-east-1 --version 1.32 --vpc-public-subnets subnet-04523687dd0d689e3,subnet-081c9e4ce171661d5 --without-nodegroup
   10  eksctl create nodegroup   --cluster my-cluster   --region us-east-1   --name my-node-group   --node-ami-family Ubuntu22.04   --node-type t2.small   --subnet-ids subnet-04523687dd0d689e3,subnet-04523687dd0d689e3   --nodes 3   --nodes-min 2   --nodes-max 4   --ssh-access   --ssh-public-key /root/.ssh/id_rsa.pub
   11  eksctl create nodegroup   --cluster my-cluster   --region us-east-1   --name my-node-group   --node-ami-family Ubuntu2204   --node-type t2.small   --subnet-ids subnet-04523687dd0d689e3,subnet-04523687dd0d689e3   --nodes 3   --nodes-min 2   --nodes-max 4   --ssh-access   --ssh-public-key /root/.ssh/id_rsa.pub
   12  eksctl create nodegroup   --cluster my-cluster   --region us-east-1   --name my-node-group   --node-ami-family Ubuntu2204   --node-type t2.small   --subnet-ids subnet-04523687dd0d689e3   --nodes 3   --nodes-min 2   --nodes-max 4   --ssh-access   --ssh-public-key /root/.ssh/id_rsa.pub
   13  kubectl get nodes
   14  kubectl get node
   15  eksctl create nodegroup   --cluster my-cluster   --region us-east-1   --name my-node-group2   --node-ami-family Ubuntu2204   --node-type t2.small   --subnet-ids subnet-04523687dd0d689e3   --nodes 3   --nodes-min 2   --nodes-max 4   --ssh-access   --ssh-public-key /root/.ssh/id_rsa.pub
   16  kubectl get nodes
   17  kubectl get pods
   18  mkdir /data
   19  cd /data
   20  vim pod.yaml
   21  kubectl apply -f pod.yaml
   22  kubectl describe nginx
   23  vim pod.yaml
   24  kubectl get pod
   25  kubectl describe pod nginx
   26  vim pod.yaml
   27  vim replicas.yaml
   28  kubectl apply -f replicas.yaml
   29  vim replicas.yaml
   30  kubectl apply -f replicas.yaml
   31  kubectl get pods
   32  vim replicas.yaml
   33  vim deployment.yaml
   34  kubectl apply -f deployment.yaml
   35  kubectl get pods
   36  kubectl describe rs
   37  vim deployment.yaml
   38  kubectl apply -f deployment.yaml
   39  kubectl get pods
   40  watch kubectl get pods
   41  history
   42  watch kubectl get pods

     vim deployment.yaml
    6  kubectl apply -f deployment.yaml
    7  kubectl get namespace
    8  kubectl create namespace prod
    9  kubectl get pod -n kube-public
   10  kubectl get pod -n default
   11  kubectl apply -f pod.yaml -n prod
   12  kubectl get pod -n prod
   13  kubectl get rs
   14  kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1
   15  kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause="image updated to 1.16.1"
   16  kubectl rollout history deployment/nginx-deployment
   17  kubectl rollout history deployment/nginx-deployment --revision=1
   18  kubectl rollout status deployment/nginx-deployment
   19  kubectl rollout history deployment/nginx-deployment
   20  kubectl rollout undo deployment/nginx-deployment
   21  kubectl rollout history deployment/nginx-deployment
   22  vim deployment.yaml
   23  kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause="image updated to 1.16.0"
   24  kubectl rollout history deployment/nginx-deployment
   25  kubectl rollout undo deployment/nginx-deployment --to-revision=3
   26  vim deployment.yaml
   27  kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause="replicas updated to 6"
   28  kubectl rollout history deployment/nginx-deployment
   29  kubectl rollout undo deployment/nginx-deployment --to-revision=4
   30  vim deployment.yaml
   31*
   32  kubectl rollout undo deployment/nginx-deployment --to-revision=4
   33  vim deployment.yaml
   34  kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause="replicas updated to 6,.1"
   35  kubectl rollout history deployment/nginx-deployment
   36  kubectl rollout undo deployment/nginx-deployment --to-revision=5
   37  vim deployment.yaml


 -----------------------for nginx -----------------------------------------------------------------------
regapp-deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: regapp-deployment
  labels:
     app: regapp

spec:
  replicas: 2
  selector:
    matchLabels:
      app: regapp

  template:
    metadata:
      labels:
        app: regapp
    spec:
      containers:
      - name: regapp
        image: nginx
        imagePullPolicy: Always
        ports:
        - containerPort: 80
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1

 ----------------------------service.yml--------------------------
 apiVersion: v1
kind: Service
metadata:
  name: regapp-service
  labels:
    app: regapp
spec:
  selector:
    app: regapp

  ports:
    - port: 80
      targetPort: 80

  type: LoadBalancer

 ###############################################################################################################################################################
 #########################################################################################################################################################################

 TERRAFORM



  
mkdir project
    2  cd project
    3  vim server.tf
    4  terraform init
    5  cd
    6  sudo dnf install -y dnf-plugins-core
    7  sudo dnf config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
    8  sudo dnf -y install terraform
    9  curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
   10  unzip awscliv2.zip
   11  sudo ./aws/install
   12  terraform --version
   13  cd project
   14  ll
   15  terraform init
   16  vim server.tf
   17  terraform fmt
   18  terraform validate
   19  terraform plan
   20  terraform apply
----------------------------------------------------------------------------------------------=----------------------------------------------------------------------------
 this is for adding the inbound and outbound rules
 vim server.tf
 provider "aws" {
  region     = "us-east-1"
  access_key = ""
  secret_key = ""
}
 resource "aws_instance" "web-server" {
  ami                    = "ami-05ffe3c48a9991133"
  instance_type          = "t2.micro"
  key_name               = "lti-mahape-key"
  availability_zone      = "us-east-1a"
  security_groups = ["${aws_security_group.web-access.name}"]
  tags = {
    Name = "hello"
  }
}

resource "aws_security_group" "web-access" {
  name        = "web-access"
  description = "Allow access"
  tags = {
    Name = "web-access"
  }
}

resource "aws_vpc_security_group_ingress_rule" "allow_http" {
  security_group_id = aws_security_group.web-access.id
  cidr_ipv4         = "0.0.0.0/0"
  from_port         = 80
  ip_protocol       = "tcp"
  to_port           = 80
}
resource "aws_vpc_security_group_ingress_rule" "allow_ssh" {
  security_group_id = aws_security_group.web-access.id
  cidr_ipv4         = "0.0.0.0/0"
  from_port         = 22
  ip_protocol       = "tcp"
  to_port           = 22
}
resource "aws_vpc_security_group_ingress_rule" "allow_443" {
  security_group_id = aws_security_group.web-access.id
  cidr_ipv4         = "0.0.0.0/0"
  from_port         = 443
  ip_protocol       = "tcp"
  to_port           = 443
}
resource "aws_vpc_security_group_egress_rule" "allow_all_traffic_ipv4" {
  security_group_id = aws_security_group.web-access.id
  cidr_ipv4         = "0.0.0.0/0"
  ip_protocol       = "-1"
}



---------------------------------------------------------------------------------------------------------------------------------------------------------------------------


This is related to the attaching the volume to the instance

 provider "aws" {
  region     = "us-east-1"
  access_key = ""
  secret_key = ""
}
resource "aws_security_group" "web-access3" {
  name        = "web-access3"
  description = "Allow access"
  tags = {
    Name = "web-access3"
  }
}

resource "aws_vpc_security_group_ingress_rule" "allow_http1" {
  security_group_id = aws_security_group.web-access3.id
  cidr_ipv4         = "0.0.0.0/0"
  from_port         = 80
  ip_protocol       = "tcp"
  to_port           = 80
}
resource "aws_instance" "creating_vol" {
  ami               = "ami-05ffe3c48a9991133"
  availability_zone = "us-east-1a"
  instance_type     = "t2.micro"
  security_groups   = ["${aws_security_group.web-access3.name}"]
  key_name          = "lti-mahape-key"
  #root disk
  root_block_device {
    volume_size           = "25"
    volume_type           = "gp2"
    delete_on_termination = true
  }
 
  #additional data disk
  ebs_block_device {
    device_name           = "/dev/xvdb"
    volume_size           = "10"
    volume_type           = "gp2"
    delete_on_termination = true
  }

  user_data = <<-EOF
        #!/bin/bash
        sudo yum install httpd -y
        sudo systemctl start httpd
        sudo systemctl enable httpd
        echo "<h1>sample webserver using terraform</h1>" | sudo tee /var/www/html/index.html
  EOF

  tags = {
    Name     = "hello-India"
    Stage    = "testing"
    Location = "India"
  }

}

vim lti-mahape-key.pem  -  paste the key of the lti-mahape-key.pem
chmod 400 lti-mahape-key.pem
   paste the connect ssh to connect the new instance 
   sudo su -
   lsblk
   mkfs.ext4 /dev/xvdb
   mkdir /data
   mount /dev/xvdb /data
   cd /data
   touch sri.txt{1..5}
   ll






   resource "aws_security_group" "web_access23" {
  name        = "web_access23"
  description = "Allow traffic"
 
  tags = {
    Name = "web_access23"
  }
}
 
resource "aws_vpc_security_group_ingress_rule" "allow_http" {
  security_group_id = aws_security_group.web_access23.id
  cidr_ipv4         = "0.0.0.0/0"
  from_port         = 80
  ip_protocol       = "tcp"
  to_port           = 80
}
 
resource "aws_vpc_security_group_ingress_rule" "allow_ssh" {
  security_group_id = aws_security_group.web_access23.id
  cidr_ipv4         = "0.0.0.0/0"
  from_port         = 22
  ip_protocol       = "tcp"
  to_port           = 22
}
 
resource "aws_vpc_security_group_ingress_rule" "allow_https" {
  security_group_id = aws_security_group.web_access23.id
  cidr_ipv4         = "0.0.0.0/0"
  from_port         = 443
  ip_protocol       = "tcp"
  to_port           = 443
}
resource "aws_vpc_security_group_egress_rule" "allow_all_taraffic" {
  security_group_id = aws_security_group.web_access23.id
  cidr_ipv4         = "0.0.0.0/0"
  ip_protocol       = "-1"
}
 
#ec2 instance code starts here
resource "aws_instance" "custom-server" {
  ami               = "ami-05ffe3c48a9991133"
  availability_zone = "us-east-1a"
  instance_type     = "t2.micro"
  security_groups   = ["${aws_security_group.web_access23.name}"]
  key_name          = "Milestone-key"
 
 
  user_data = <<-EOF
        #!/bin/bash
        sudo yum install httpd -y
        sudo systemctl start httpd
        sudo systemctl enable httpd
        echo "<h1>custom  webserver using terraform</h1>" | sudo tee /var/www/html/index.html
  EOF
  tags = {
    Name     = "hello-India"
    Stage    = "testing"
    Location = "India"
  }
}
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
   


for using the key paie that is created for security 
   provider "aws" {
  region     = "us-east-1"
  access_key = ""
  secret_key = ""
}

resource "aws_instance" "creating_vol" {
  ami               = "ami-05ffe3c48a9991133"
  availability_zone = "us-east-1a"
  instance_type     = "t2.micro"

  key_name = "deployer-key"
  #root disk
  root_block_device {
    volume_size           = "25"
    volume_type           = "gp2"
    delete_on_termination = true
  }

  #additional data disk
  ebs_block_device {
    device_name           = "/dev/xvdb"
    volume_size           = "10"
    volume_type           = "gp2"
    delete_on_termination = true
  }
   
  user_data = <<-EOF
        #!/bin/bash
        sudo yum install httpd -y
        sudo systemctl start httpd
        sudo systemctl enable httpd
        echo "<h1>sample webserver using terraform</h1>" | sudo tee /var/www/html/index.html
  EOF

  tags = {
    Name     = "hello-India"
    Stage    = "testing"
    Location = "India"
  }

}

resource "aws_key_pair" "deployer" {
  key_name   = "deployer-key"
  public_key = ""
}   



for using existing security group 
 provider "aws" {
  region     = "us-east-1"
  access_key = ""
  secret_key = ""
}
resource "aws_instance" "creating_vol" {
  ami                    = "ami-05ffe3c48a9991133"
  availability_zone      = "us-east-1a"
  instance_type          = "t2.micro"
  vpc_security_group_ids = [data.aws_security_group.previous-sg.id]
  key_name               = "lti-mahape-key"
  tags = {
    Name     = "existing_sg_inst"
    Stage    = "testing"
    Location = "India"
  }
}

data "aws_security_group" "previous-sg" {
  id = "sg-0bf9399b22dafa7fc" #existing security group id
}




this is for provisioning automatically
   provider "aws" {
  region     = "us-east-1"
  access_key = ""
  secret_key = ""
}

data "aws_security_group" "previous-sg" {
  id = "sg-0bf9399b22dafa7fc" #existing security group id
}
data "aws_ami" "ubuntu" {
  most_recent = true

  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"]
  }

  filter {
    name   = "virtualization-type"              #Terraform to only look for AMIs that use HVM (Hardware Virtual Machine) virtualization.
    values = ["hvm"]
  }

  owners = ["099720109477"] # Canonical             #This restricts the search to AMIs owned by a specific AWS account.
}

resource "aws_instance" "web" {
  ami                    = data.aws_ami.ubuntu.id
  instance_type          = "t2.micro"
  availability_zone      = "us-east-1a"
  key_name               = "lti-mahape-key"
  vpc_security_group_ids = [data.aws_security_group.previous-sg.id]
  tags = {
    Name = "HelloWorld"
  }
}

   
 
mkdir project
    2  cd project
    3  vim server.tf
    4  terraform init
    5  cd
    6  sudo dnf install -y dnf-plugins-core
    7  sudo dnf config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
    8  sudo dnf -y install terraform
    9  curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
   10  unzip awscliv2.zip
   11  sudo ./aws/install
   12  terraform --version
   13  cd project
   14  ll
   15  terraform init
   16  vim server.tf
   17  terraform fmt
   18  terraform validate
   19  terraform plan
   20  terraform apply
 21

   ###Create AWS AMI from AWS INStance
resource "aws_ami_from_instance" "my-images" {
  name               = "terra-image"
  source_instance_id = "i-08378f82f606e2542"

tags = {
    Name = "Hello-world"
  }
   }

------------------------------------------------------------------------------------------------------------------------------------------------------------------------
   for vpc creation 
 provider "aws" {
  region     = "us-east-1"
  access_key = ""
  secret_key = ""
}

resource "aws_vpc" "test-vpc" {
  cidr_block = "10.0.0.0/16"
}
resource "aws_subnet" "public-subnet" {
  vpc_id            = aws_vpc.test-vpc.id
  availability_zone = "us-east-1a"
  cidr_block        = "10.0.0.0/24"

  tags = {
    Name = "Public-subnet"
  }
}
resource "aws_subnet" "private-subnet" {
  vpc_id            = aws_vpc.test-vpc.id
  availability_zone = "us-east-1b"
  cidr_block        = "10.0.1.0/24"

  tags = {
    Name = "Private-subnet" #security group
  }
}
resource "aws_security_group" "test_access" {
  name        = "test_access"
  vpc_id      = aws_vpc.test-vpc.id
  description = "allow ssh and http"

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }


}
resource "aws_internet_gateway" "test-igw" {
  vpc_id = aws_vpc.test-vpc.id

  tags = {
    Name = "test-igw"
  }
}

resource "aws_route_table" "public-rt" {
  vpc_id = aws_vpc.test-vpc.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.test-igw.id
  }


  tags = {
    Name = "public-rt"

  }
}
resource "aws_route_table_association" "public-asso" {
  subnet_id      = aws_subnet.public-subnet.id
  route_table_id = aws_route_table.public-rt.id
}
resource "aws_instance" "sri-server" {
  ami             = "ami-05ffe3c48a9991133"
  subnet_id       = aws_subnet.public-subnet.id
  instance_type   = "t2.micro"
  security_groups = ["${aws_security_group.test_access.id}"]
  key_name        = "ltimindtree"
  tags = {
    Name     = "test-World"
    Stage    = "testing"
    Location = "chennai"
  }

}
resource "aws_eip" "sri-ec2-eip" {
  instance = aws_instance.sri-server.id
}
resource "aws_key_pair" "ltimindtree" {
  key_name   = "ltimindtree"
  public_key = "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDsiVHsGsczvFyiEezuKaVb60WPaWJ7l+9HiE6qMOAVY0qftZZx8CRRtnwPsHt6bSBTTfM5P5e4nKZF7AWG+qU2Mr1zjVOlHJHSJOi9StJrqEOwZMj/xkzZ1KDuFahpFHYTDYUR/kjPiZfOEyrsuaCVB6KwyidnseJjG/GJwrLOCSro2/dEfGWPS8k/b/UAUiarGpavD+kWqCQfNOfrfAiLkQztaUZiT7uHUQ9TmmEUXqNliP0B7pnXEHodi/E3xn91/3nWVVvklyCYfpx7r2XEteEq7imGrP5U9Y4G5VB7o/L9ZDcjXDuoCkEtnMGOuXwJgp5n3PaIippiM7sPWl79q4GcRVg+7j+aET1J5Edda6e99g9ukjLcQts7+rESQEyzp28T6opcmF5wvvhLP0r1DwwFCv6LXX1K3QDrmYr48M+gWIeCi7MWePZfeP5u+jFjGTKoHvgQrt0Rpm+VixgYM91WTqiWjklFs7ArVGczR5+XZcCz1/6AeszPzF7Cwr8= root@ip-172-31-40-165.ec2.internal"
}
resource "aws_instance" "database-server" {
  ami             = "ami-05ffe3c48a9991133"
  subnet_id       = aws_subnet.private-subnet.id
  instance_type   = "t2.micro"
  security_groups = ["${aws_security_group.test_access.id}"]
  key_name        = "ltimindtree"
  tags = {
    Name     = "db-World"
    Stage    = "stage-base"
    Location = "delhi"
  }

}
resource "aws_eip" "nat-eip" {
  tags = {
    Name = "nat-eip"
  }
}
resource "aws_nat_gateway" "my-ngw" {
  allocation_id = aws_eip.nat-eip.id
  subnet_id     = aws_subnet.public-subnet.id
}

resource "aws_route_table" "private-rt" {
  vpc_id = aws_vpc.test-vpc.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_nat_gateway.my-ngw.id
  }


  tags = {
    Name = "private-rt"
  }
}
resource "aws_route_table_association" "private-asso" {
  subnet_id      = aws_subnet.private-subnet.id
  route_table_id = aws_route_table.private-rt.id
}





    using variables

   provider "aws" {
  region     = "us-east-1"
  access_key = ""
  secret_key = ""
}
variable "vpc_cidr" {
  default     = "10.1.0.0/16"
  description = "cidr for our custom vpc"
}

variable "subnet_cidr" {
  default     = "10.1.1.0/24"
  description = "cidr for subnet"
}

variable "availability_zone" {
  default     = "us-east-1a"
  description = "AZ for subnet"
}

variable "instance_ami" {
  default     = "ami-05ffe3c48a9991133"
  description = "default ami for instances"
}

variable "instance_type" {
  default     = "t2.micro"
  description = "instance type for ec2"
}
   variable "env_tag" {
  default     = "production"
  description = "environment tag"
}


# code - creating vpc

resource "aws_vpc" "vpcone" {
  cidr_block = var.vpc_cidr
  tags = {
    Name = "${var.env_tag}"
  }
}

# code - creating IG and attaching it to VPC

resource "aws_internet_gateway" "vpcone-ig" {
  vpc_id = aws_vpc.vpcone.id
  tags = {
    Name = "${var.env_tag}"
  }
}
resource "aws_subnet" "subnet_public" {
  vpc_id                  = aws_vpc.vpcone.id
  cidr_block              = var.subnet_cidr
  map_public_ip_on_launch = "true"
  availability_zone       = var.availability_zone
  tags = {
    Name = "${var.env_tag}"
  }

}

# code - modifying route

resource "aws_route_table" "rtb_public" {
  vpc_id = aws_vpc.vpcone.id
  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.vpcone-ig.id
  }
  tags = {
    Name = "${var.env_tag}"
  }
}
resource "aws_route_table_association" "rta_subnet_public" {
  subnet_id      = aws_subnet.subnet_public.id
  route_table_id = aws_route_table.rtb_public.id
}
resource "aws_security_group" "sg_newvpc" {
  name   = "newvpc"
  vpc_id = aws_vpc.vpcone.id

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "${var.env_tag}"
  }

}
   
resource "aws_instance" "test" {
  ami                    = var.instance_ami
  instance_type          = var.instance_type
  subnet_id              = aws_subnet.subnet_public.id
  vpc_security_group_ids = ["${aws_security_group.sg_newvpc.id}"]
  tags = {
    Name = "${var.env_tag}"
  }
}

#########################################################################################################################################
##########################################################################################################################

   Github actions
   
   open github - create a new repo - create a new file ( add some content in that and give it a name index.html)
   in aws console - create a bucket
                    enable static webhosting - (index.html ) - click on create 
                    permissions policies 
 {
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PublicReadForGetBucketObjects",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::<bucket name>/*"
        }
    ]
}
     in github repo open variables and environments add access key and secret key 
             in repo add a new file and in the top place .github/workflows/main.yml
name: Deploy to AWS S3
on:
  push:
    branches:
      - main

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v1

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Deploy static site to S3 bucket
        run: aws s3 sync . s3://<bucket-name> --delete
             
   
 
 
 
 
                
    
 
                                      
